# 6.3.2022
# When selecting high performing trees small forests should have an advantage over regular small forests.
# can we visualize that?

# plot logloss over num.trees for the regular forest and the forest of high performers.

# plot their difference , plot the mean over simulations.

# result: the difference is (in the mean) monotonously decreasing, very steep in the beginning and then very flat.
# we could plot this difference against the total logloss, maybe there would be a biggest relative difference somewhere in the middle

rm(list=ls())

library(dplyr)
library(caret)
library(ranger)
#library(effects)
library(cluster)

source('code/source/prep.R') # calcLogloss (on forests) , calcLogloss2 (on predicted probabilities)
source('code/source/subforest.R') # subforest

source('code/source/chipman.R')

load('data/data_SupMat4.rda') # loads the data sets Cleve, Hung, Swiss, VA

# set test data by name : VA, Swiss or Hung
data.test.name <-  'Swiss'
data.test <-  get(data.test.name)
attr(data.test,'data.test.name') <- data.test.name

calc_smthn <- function(doc){
  
  nBs <- length(doc)
  #nBs <- 5
  
  stepSize <-  5
  # evalT <- array(NA,dim=c(nBs,2 , 50)) # table of evaluations , for step size 10
  evalT <- array(NA,dim=c(nBs,2 , 500/stepSize)) 
  
  ct <- 1
  for(i in 1:nBs){
    #print(paste(i/nBs , Sys.time()))
    if(i%%10 ==0) print(paste(i/nBs , Sys.time()))
    
    forest<-doc[[i]]$rg$forest
    
    data.train <- doc[[i]]$`bootstapped training data`

    OOB <-  base::setdiff(1:nrow(Cleve), unique(doc[[i]]$resample))
    data.set.val <- Cleve[OOB,] # goes back to original Cleveland data
    
    #pp <- predict(forest 
    #          , data=data.set.val 
    #          , predict.all = T)$predictions[,2,]
    #pp <- simplify2array(pp, higher=FALSE)

    # is this the same as Vectorize ??!!
    #lapply(1:forest$num.trees
    #   , function(k){ 
    #     pp[,k] %>% 
    #       calcLogloss2( df=data.set.val ) %>% 
    #       unlist
    #     # unlist(calcLogloss2(pp=pp[,k] , df=data.set.val ) ) 
    #   }) %>% 
    #unlist -> LL
    
    #hpst <- order(LL)[1:sizeSF] # high performing single trees for second basecase
    oLL <-  calc_oLL(forest, data.set.val)
    #print(oLL)

    # building the forest sequentially, adding trees the way they were generated by ranger
    evalT[i,1,] <- lapply(seq(stepSize,500,stepSize),
                        function(k) calcLogloss( subforest(forest, 1:k), data.test)) %>% unlist
    # builing the forest starting with the best (lowest logloss on OOB Cleveland) and always adding the best of the remaining group
    evalT[i,2,] <- lapply(seq(stepSize,500,stepSize),
                          function(k) calcLogloss( subforest(forest, oLL[1:k] ), data.test) ) %>% unlist
    #print(calcLogloss( subforest(forest, 1:500), data.test)== calcLogloss( forest, data.test) )
    #print( calcLogloss( forest,  data.test ) - calcLogloss( subforest(forest, oLL[1:500] ), data.test) ) # around (1e-16, +/- 5e-17)
    
  }
  return(evalT)
}

# to base the result on more bootstraps
folder <- 'data/nursery'
files <- list.files(folder)[1:1]
# dir(folder)
collector <-  list()
ct <-  1 # counter for the above collector

for(file in files){
  # run loops over doc loaded from file
  load(paste(folder,file, sep='/'))
  collector[[ct]] <- calc_smthn(doc)
  ct <-  ct+1
}
# et <- bind_rows(collector)

(collector[[1]][1,1,]-collector[[1]][1,2,]) %>% plot()

et <-  collector[[1]]
k <- 3
plot(2:50, et[k,1,2:50] , type='l' , ylim=range(et[k,,2:50]))
points(2:50, et[k,2,2:50] , type='l', col='blue')
legend('topright', 
       legend=c('regular forest, size increasing','high performing trees first') , 
       col=c('black','blue') ,
       pch = '-' ,
       cex =0.8)

# in et : dimensions : simulation x logloss for regular small forest or subforest of high performers x number of trees in forest
# this is what we want for a fixed simulation
et[3,,] %>% (function(A2dim) A2dim[1,] - A2dim[2,] ) %>% plot(type='b')

A <-  list()
# A will contain vectors of length 500/stepSize, i.e. for stepSize 5 : length is 100
#  each list in A is the difference in logloss when selecting sequential subforests 1:k vs best trees oLL[1:k]
for(i in 1:20){
  et <-  collector[[i]]
  # we fix the simulation, 1.st dimension
  apply(et,1,(function(A2dim) A2dim[1,] - A2dim[2,] )) %>% # each column is a result of apply, with a fixed simulation
    apply(1,mean) -> A[[i]] # take mean over columns, always with a fixed row , which is a forest size
  plot(A[[i]], type='l', main=i)
}
X <-  simplify2array(A)
apply(X,1,mean) %>% # taking mean over the packages of 50 simulations
  plot(type='p') 
X[1:10]

